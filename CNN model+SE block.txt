import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import GlobalAveragePooling2D, Flatten, Dense, Dropout
from tensorflow.keras.applications import Xception
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
import numpy as np
import time
from tensorflow.keras.layers import Layer, GlobalAveragePooling2D, Dense, Multiply, Reshape

class SEBlock(Layer):
    def __init__(self, channels, reduction=16):
        super(SEBlock, self).__init__()
        self.global_avg_pool = GlobalAveragePooling2D()
        self.dense1 = Dense(channels // reduction, activation='relu', kernel_initializer='he_normal', use_bias=False)
        self.dense2 = Dense(channels, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)
        self.reshape = Reshape((1, 1, channels))
        self.multiply = Multiply()

    def call(self, inputs):
        x = self.global_avg_pool(inputs)
        x = self.dense1(x)
        x = self.dense2(x)
        x = self.reshape(x)
        return self.multiply([inputs, x])



# Create the base model with pre-trained weights (Xception, InceptionV3, MobileNetV2)
base_model = Xception(weights='imagenet', include_top=False, input_shape=(299, 299, 3))  

# Freeze the base model
for layer in base_model.layers:
    layer.trainable = False

# Integrate SE blocks after specific layers in the Xception model
x = base_model.get_layer('block1_conv1_act').output
x = SEBlock(x.shape[-1])(x)

x = base_model.get_layer('block3_sepconv1_act').output
x = SEBlock(x.shape[-1])(x)

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Flatten()(x)
x = Dense(units=1024, activation='relu')(x)
x = Dropout(0.3)(x)
output = Dense(units=2, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=output)
model.summary()

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Use the Image Data Generator to import the images from the dataset
train_datagen = ImageDataGenerator(rescale=1./255, 
                                rotation_range=40,
                                width_shift_range=0.2,
                                height_shift_range=0.2,
                                shear_range=0.2,
                                zoom_range=0.2,
                                horizontal_flip=True,
                                fill_mode='nearest')

valid_datagen = ImageDataGenerator(rescale=1./255)

# Make sure you provide the same target size as initialized for the image size
train_set = train_datagen.flow_from_directory('Fig_dataset/train',
                                              target_size=(299, 299),
                                              batch_size=32,
                                              class_mode='categorical')

valid_set = valid_datagen.flow_from_directory('Fig_dataset/test',
                                              target_size=(299, 299),
                                              batch_size=32,
                                              class_mode='categorical',
                                              shuffle=False)

# Compile the model
model.compile(loss="categorical_crossentropy",
              optimizer=Adam(learning_rate=0.001),
              metrics=['accuracy'])

# Define callbacks
checkpoint = ModelCheckpoint('xception_se_block.h5', monitor='val_loss', save_best_only=True)
early_stop = EarlyStopping(monitor='val_loss', patience=10)

# Measure training time
start_time = time.time()

# Train the model
r = model.fit(train_set,
              validation_data=valid_set,
              epochs=30,
              steps_per_epoch=len(train_set),
              validation_steps=len(valid_set),
              callbacks=[checkpoint, early_stop])

training_time = time.time() - start_time
print(f"Training time: {training_time} seconds")

import numpy as np
from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, cohen_kappa_score, roc_auc_score

# Measure test time
start_time = time.time()

# Make predictions on the test set
y_pred = model.predict(valid_set)

test_time = time.time() - start_time
print(f"Test time: {test_time} seconds")

# Convert the predicted probabilities to class labels
y_pred_classes = np.argmax(y_pred, axis=1)

# Get the true class labels
y_true = valid_set.classes

# Confusion matrix
conf_matrix = confusion_matrix(y_true, y_pred_classes)
print("Confusion matrix:", conf_matrix)

# Accuracy
accuracy = np.mean(y_true == y_pred_classes)
print("Accuracy:", accuracy)

# Recall
recall = recall_score(y_true, y_pred_classes, average='weighted')
print("Recall:", recall)

# Precision
precision = precision_score(y_true, y_pred_classes, average='weighted')
print("Precision:", precision)

# F1 score
f1 = f1_score(y_true, y_pred_classes, average='weighted')
print("F1 score:", f1)

# Kappa
kappa = cohen_kappa_score(y_true, y_pred_classes)
print("Cohen's Kappa:", kappa)

# AUC
# Note: Since this is a multi-class classification, we need to binarize the labels for AUC calculation.
y_true_bin = tf.keras.utils.to_categorical(y_true, num_classes=2)
auc = roc_auc_score(y_true_bin, y_pred, average='weighted', multi_class='ovr')
print("AUC:", auc)

import matplotlib.pyplot as plt

# Plot the loss
plt.plot(r.history['loss'], label='train loss')
plt.plot(r.history['val_loss'], label='val loss')
plt.legend()
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.grid()

# Set y-axis limits
ax = plt.gca()
ax.set_ylim(0, 1)
plt.show()
plt.savefig('LossVal_loss')

# Plot the accuracy
plt.plot(r.history['accuracy'], label='train acc')
plt.plot(r.history['val_accuracy'], label='val acc')
plt.legend()
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.grid()

# Set y-axis limits
ax = plt.gca()
ax.set_ylim(0, 1)
plt.show()
plt.savefig('AccVal_acc')

from sklearn import metrics

# Generate confusion matrix
confusion_matrix = metrics.confusion_matrix(y_true, y_pred_classes)

# Create ConfusionMatrixDisplay object
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=[False, True])

# Plot the confusion matrix
fig, ax = plt.subplots()
cm_display.plot(ax=ax)  # Pass the axes object to the plot function

# Set the tick labels font size
ax.tick_params(axis='both', labelsize=20)

# Set the x-axis and y-axis tick labels
tick_marks = np.arange(2)
plt.xticks(tick_marks, ['healthy', 'infected'], rotation=45, fontsize=14)
plt.yticks(tick_marks, ['healthy', 'infected'], fontsize=14)

# Set the axis labels font size
plt.xlabel('Predicted label', fontsize=14)
plt.ylabel('True label', fontsize=14)

# Get the text objects for the result values
text_objects = ax.texts

# Set the font size for the result values
for text_obj in text_objects:
    if text_obj.get_text().isdigit():  # Check if the text is a digit (result value)
        text_obj.set_fontsize(20)  # Set the desired font size

# Adjust the plot layout
plt.tight_layout()

# Show the plot
plt.show()

